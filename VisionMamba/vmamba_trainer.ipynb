{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfqPRqnbg4QK",
        "outputId": "7d75d3e9-2078-42db-fa6b-f7c55053c201"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.20)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm) (6.0.3)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm) (0.35.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm) (0.6.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (1.1.10)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2025.10.5)\n",
            "Cloning into 'VMamba'...\n",
            "remote: Enumerating objects: 8910, done.\u001b[K\n",
            "remote: Counting objects: 100% (1398/1398), done.\u001b[K\n",
            "remote: Compressing objects: 100% (175/175), done.\u001b[K\n",
            "remote: Total 8910 (delta 1341), reused 1223 (delta 1223), pack-reused 7512 (from 2)\u001b[K\n",
            "Receiving objects: 100% (8910/8910), 34.84 MiB | 17.16 MiB/s, done.\n",
            "Resolving deltas: 100% (5492/5492), done.\n"
          ]
        }
      ],
      "source": [
        "# Gerekli k\u00fct\u00fcphaneler\n",
        "!pip install timm scikit-learn numpy pandas tqdm torch torchvision\n",
        "\n",
        "# VMamba reposunu klonla\n",
        "# Not: Bu ad\u0131m, e\u011fer Colab ortam\u0131nda modeliniz y\u00fckl\u00fc de\u011filse gereklidir.\n",
        "# VMamba_v10.pth modelini manuel olarak Drive'a y\u00fcklediyseniz bu sat\u0131ra gerek kalmayabilir.\n",
        "!git clone https://github.com/MzeroMiko/VMamba.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Bu k\u0131s\u0131m sadece Colab'da \u00e7al\u0131\u015f\u0131r\n",
        "try:\n",
        "    print(\"Google Drive'a ba\u011flan\u0131l\u0131yor...\")\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Drive ba\u011flant\u0131s\u0131 ba\u015far\u0131l\u0131.\")\n",
        "\n",
        "    # Veri setinizin Google Drive'daki yolu\n",
        "    # L\u00fctfen 'new_data' klas\u00f6r\u00fcn\u00fcn do\u011fru yolda oldu\u011fundan emin olun\n",
        "    DATA_DIR = '/content/drive/MyDrive/data_v1'\n",
        "\n",
        "except ImportError:\n",
        "    # Colab d\u0131\u015f\u0131nda \u00e7al\u0131\u015f\u0131yorsan\u0131z bu yol kullan\u0131l\u0131r\n",
        "    print(\"Colab ortam\u0131 alg\u0131lanmad\u0131. Yerel yollar kullan\u0131l\u0131yor.\")\n",
        "    DATA_DIR = './data_v1'\n",
        "\n",
        "# Veri klas\u00f6r\u00fcn\u00fcn varl\u0131\u011f\u0131n\u0131 kontrol et\n",
        "if not os.path.exists(DATA_DIR):\n",
        "    raise FileNotFoundError(f\"Hata: '{DATA_DIR}' yolu bulunamad\u0131. L\u00fctfen Google Drive'daki veri klas\u00f6r\u00fcn\u00fcn do\u011fru konumda oldu\u011fundan emin olun.\")\n",
        "else:\n",
        "    print(f\"Veri klas\u00f6r\u00fc ba\u015far\u0131yla bulundu: {DATA_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IJcPUtVhAgE",
        "outputId": "2c270fa6-a273-41f1-8141-01302a80edf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google Drive'a ba\u011flan\u0131l\u0131yor...\n",
            "Mounted at /content/drive\n",
            "Drive ba\u011flant\u0131s\u0131 ba\u015far\u0131l\u0131.\n",
            "Veri klas\u00f6r\u00fc ba\u015far\u0131yla bulundu: /content/drive/MyDrive/data_v1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import timm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import numpy as np\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# --- YARDIMCI FONKS\u0130YON: S\u00dcRE FORMATLAMA ---\n",
        "\n",
        "def format_duration(seconds):\n",
        "    \"\"\"Saniye cinsinden s\u00fcreyi (H saat, M dakika, S saniye) format\u0131na d\u00f6n\u00fc\u015ft\u00fcr\u00fcr.\n",
        "       \u0130stenen HH saat, MM dakika, SS saniye format\u0131 kullan\u0131l\u0131r.\"\"\"\n",
        "    seconds = int(round(seconds))\n",
        "    h = seconds // 3600\n",
        "    m = (seconds % 3600) // 60\n",
        "    s = seconds % 60\n",
        "    return f\"{h:02d} saat, {m:02d} dakika, {s:02d} saniye\"\n",
        "\n",
        "# --- YAPILANDIRMA VE YOL TANIMLARI ---\n",
        "\n",
        "# L\u00dcTFEN KONTROL ED\u0130N: Drive \u00fczerindeki verilerinizin ana klas\u00f6r yolu\n",
        "ROOT_DIR = '/content/drive/MyDrive/data_v1'\n",
        "# E\u011e\u0130T\u0130LM\u0130\u015e MODELLER\u0130N KAYDED\u0130LECE\u011e\u0130 KLAS\u00d6R YOLU\n",
        "DRIVE_MODEL_DIR = '/content/drive/MyDrive/vmamba_models'\n",
        "\n",
        "# SADECE T\u00dcRK\u00c7E (TR) \u0130\u00c7\u0130N HARF L\u0130STES\u0130\n",
        "language_info = {\n",
        "    # Bu liste alt klas\u00f6r isimlerinizle e\u015fle\u015fmelidir.\n",
        "    'TR': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'U', 'V', 'Y', 'Z'],\n",
        "}\n",
        "ALL_LANG_CODES = ['TR']\n",
        "\n",
        "# E\u011fitim parametreleri\n",
        "NUM_EPOCHS = 50 # Colab'da GPU'da bile uzun s\u00fcrebilir\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.0001\n",
        "TEST_SIZE = 0.2\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# --- MODEL VE VER\u0130 SINIFLARI ---\n",
        "\n",
        "class CustomMambaClassifier(nn.Module):\n",
        "    \"\"\"VMamba'ya benzer \u015fekilde \u00e7al\u0131\u015fan, ConvNeXt tabanl\u0131 g\u00f6r\u00fcnt\u00fc s\u0131n\u0131fland\u0131r\u0131c\u0131s\u0131.\"\"\"\n",
        "    def __init__(self, num_classes, pretrained=True):\n",
        "        super(CustomMambaClassifier, self).__init__()\n",
        "        # G\u00fc\u00e7l\u00fc bir temel olarak ConvNeXt Base kullan\u0131l\u0131yor.\n",
        "        self.backbone = timm.create_model('convnext_base', pretrained=pretrained, features_only=True)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(self.backbone.feature_info[-1]['num_chs'], self.num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Yaln\u0131zca son \u00f6zellik haritas\u0131 kullan\u0131l\u0131r\n",
        "        x = self.backbone(x)[-1]\n",
        "        x = self.avgpool(x)\n",
        "        x = self.head(x)\n",
        "        return x\n",
        "\n",
        "class SignLanguageDataset(Dataset):\n",
        "    \"\"\"\u0130\u015faret dili g\u00f6r\u00fcnt\u00fcleri i\u00e7in \u00f6zel veri k\u00fcmesi s\u0131n\u0131f\u0131.\"\"\"\n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# --- VER\u0130 HAZIRLAMA FONKS\u0130YONU ---\n",
        "\n",
        "def prepare_lang_data(root_dir, lang_code, test_size=TEST_SIZE, random_state=RANDOM_STATE):\n",
        "    \"\"\"Sadece belirtilen dilin resimlerini toplar ve etiketler.\"\"\"\n",
        "    all_image_paths = []\n",
        "    all_labels = []\n",
        "\n",
        "    lang_path = os.path.join(root_dir, lang_code)\n",
        "    if not os.path.exists(lang_path):\n",
        "        print(f\"Uyar\u0131: '{lang_code}' diline ait klas\u00f6r bulunamad\u0131. Bu dil atlan\u0131yor.\")\n",
        "        return None, None, None, None, None\n",
        "\n",
        "    # Bu d\u00f6ng\u00fc, tek el resimlerini i\u00e7eren klas\u00f6rleri (\u00f6rn: C) ve\n",
        "    # \u00e7ift el resimlerini i\u00e7eren klas\u00f6rleri (\u00f6rn: H) ay\u0131rt etmeksizin etiketleriyle toplar.\n",
        "    for char_dir in language_info.get(lang_code, []):\n",
        "        char_path = os.path.join(lang_path, char_dir)\n",
        "        if os.path.isdir(char_path):\n",
        "            label = char_dir\n",
        "            for img_name in os.listdir(char_path):\n",
        "                img_path = os.path.join(char_path, img_name)\n",
        "                if img_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    all_image_paths.append(img_path)\n",
        "                    all_labels.append(label)\n",
        "\n",
        "    print(f\"\\nVeri Kontrol\u00fc: '{lang_code}' i\u00e7in Toplam {len(all_labels)} resim bulundu.\")\n",
        "    if len(all_labels) == 0:\n",
        "        print(f\"Hata: '{lang_code}' dizininde hi\u00e7 resim bulunamad\u0131. Atlan\u0131yor.\")\n",
        "        return None, None, None, None, None\n",
        "\n",
        "    le = LabelEncoder()\n",
        "    encoded_labels = le.fit_transform(all_labels)\n",
        "\n",
        "    # Veri setini e\u011fitim ve test olarak b\u00f6lme\n",
        "    train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
        "        all_image_paths, encoded_labels, test_size=test_size, random_state=random_state, stratify=encoded_labels\n",
        "    )\n",
        "\n",
        "    # S\u0131n\u0131f haritas\u0131n\u0131 olu\u015ftur ve kaydet\n",
        "    class_map = {name: int(label) for name, label in zip(le.classes_, le.transform(le.classes_))}\n",
        "    class_map_path = os.path.join(DRIVE_MODEL_DIR, f'class_map_{lang_code}.json')\n",
        "    os.makedirs(os.path.dirname(class_map_path), exist_ok=True)\n",
        "    try:\n",
        "        with open(class_map_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(class_map, f, ensure_ascii=False, indent=4)\n",
        "            print(f\"\\n{lang_code} i\u00e7in {len(class_map)} s\u0131n\u0131f haritas\u0131 '{class_map_path}' dosyas\u0131na kaydedildi.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Uyar\u0131: S\u0131n\u0131f haritas\u0131 kaydedilemedi: {e}\")\n",
        "\n",
        "\n",
        "    return train_paths, test_paths, train_labels, test_labels, le.classes_\n",
        "\n",
        "# Veri zenginle\u015ftirme (Data Augmentation) ve normalizasyon ad\u0131mlar\u0131\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Do\u011frulama/Test i\u00e7in kullan\u0131lan transform\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# --- MODEL DE\u011eERLEND\u0130RME ---\n",
        "\n",
        "def evaluate_model(model, test_loader, device, class_names):\n",
        "    \"\"\"Verilen modelin test verisi \u00fczerindeki performans\u0131n\u0131 de\u011ferlendirir.\"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    test_bar = tqdm(test_loader, desc=\"[Test]\", leave=False)\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_bar:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    report = classification_report(all_labels, all_preds, target_names=class_names, zero_division=0)\n",
        "\n",
        "    return accuracy, report\n",
        "\n",
        "# --- TEK D\u0130L E\u011e\u0130T\u0130M FONKS\u0130YONU ---\n",
        "\n",
        "def train_single_language_model(lang_code, train_paths, test_paths, train_labels, test_labels, class_names):\n",
        "    \"\"\"Tek bir dil i\u00e7in modeli e\u011fitir, kaydeder ve raporlar.\"\"\"\n",
        "\n",
        "    print(f\"\\n{'='*20} {lang_code} D\u0130L\u0130 \u0130\u00c7\u0130N E\u011e\u0130T\u0130M BA\u015eLIYOR ({len(class_names)} S\u0131n\u0131f) {'='*20}\")\n",
        "\n",
        "    epoch_log_path = os.path.join(DRIVE_MODEL_DIR, f'vmamba_{lang_code}_epoch_details.txt')\n",
        "\n",
        "    train_dataset = SignLanguageDataset(train_paths, train_labels, transform)\n",
        "    test_dataset = SignLanguageDataset(test_paths, test_labels, val_transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "    num_classes = len(class_names)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = CustomMambaClassifier(num_classes).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Log dosyas\u0131 i\u00e7in ba\u015fl\u0131k yazma\n",
        "    with open(epoch_log_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(f\"--- VMAMBA E\u011e\u0130T\u0130M DETAYLI EPOCH LOGLARI ({lang_code}) ---\\n\")\n",
        "        f.write(f\"Ba\u015flang\u0131\u00e7 Tarih/Saat: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "        f.write(f\"Veri Seti Boyutu (E\u011fitim): {len(train_dataset)}, (Test): {len(test_dataset)}\\n\")\n",
        "        f.write(f\"Toplam S\u0131n\u0131f Say\u0131s\u0131: {num_classes}\\n\")\n",
        "        f.write(\"-\" * 50 + \"\\n\\n\")\n",
        "\n",
        "\n",
        "    # --- E\u011e\u0130T\u0130M D\u00d6NG\u00dcS\u00dc (COLAB'DA UZUN S\u00dcRE B\u0130L\u0130R!) ---\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        epoch_start_time = time.time()\n",
        "        epoch_start_datetime = datetime.now().strftime(\"%H:%M:%S\")\n",
        "\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [{lang_code} E\u011fitimi]\", leave=False)\n",
        "        for images, labels in train_bar:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            train_bar.set_postfix({'Kay\u0131p': running_loss / (train_bar.n + 1)})\n",
        "\n",
        "        avg_train_loss = running_loss / len(train_loader)\n",
        "\n",
        "        # --- EPOCH SONU DE\u011eERLEND\u0130RME ---\n",
        "\n",
        "        overall_accuracy, overall_report = evaluate_model(model, test_loader, device, class_names)\n",
        "\n",
        "        epoch_end_time = time.time()\n",
        "        epoch_end_datetime = datetime.now().strftime(\"%H:%M:%S\")\n",
        "        epoch_duration = epoch_end_time - epoch_start_time\n",
        "\n",
        "        # S\u00fcreyi HH saat, MM dakika, SS saniye format\u0131na d\u00f6n\u00fc\u015ft\u00fcr\n",
        "        formatted_epoch_duration = format_duration(epoch_duration)\n",
        "\n",
        "        # --- MODEL VE LOG KAYDETME ---\n",
        "\n",
        "        # 1. Modeli kaydet (pth)\n",
        "        model_filename = f'vmamba_{lang_code}_epoch_{epoch+1}.pth'\n",
        "        model_path = os.path.join(DRIVE_MODEL_DIR, model_filename)\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "        print(f\"\\nModel, '{model_path}' olarak kaydedildi.\")\n",
        "\n",
        "        # 2. Detayl\u0131 Epoch Raporunu kaydet (txt)\n",
        "        with open(epoch_log_path, 'a', encoding='utf-8') as f:\n",
        "            f.write(f\"| --- EPOCH {epoch+1}/{NUM_EPOCHS} Raporu ({lang_code}) ---\\n\")\n",
        "            f.write(f\"| Ba\u015flang\u0131\u00e7 Saati: {epoch_start_datetime}\\n\")\n",
        "            f.write(f\"| Biti\u015f Saati: {epoch_end_datetime}\\n\")\n",
        "            f.write(f\"| S\u00fcre: {formatted_epoch_duration}\\n\") # \u0130STENEN FORMAT\n",
        "            f.write(f\"| Ortalama E\u011fitim Kayb\u0131: {avg_train_loss:.4f}\\n\")\n",
        "            f.write(f\"| Test Do\u011frulu\u011fu: {overall_accuracy:.4f}\\n\")\n",
        "            f.write(\"| S\u0131n\u0131fland\u0131rma Raporu:\\n\")\n",
        "\n",
        "            for line in overall_report.split('\\n'):\n",
        "                 f.write(f\"| {line.strip()}\\n\")\n",
        "\n",
        "            f.write(\"| ------------------------------------------------------------------\\n\\n\")\n",
        "\n",
        "        print(f\"Epoch {epoch+1} tamamland\u0131. Do\u011fruluk: {overall_accuracy:.4f}. Detayl\u0131 rapor '{epoch_log_path}' dosyas\u0131na eklendi.\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "\n",
        "    # --- D\u0130L BAZLI B\u0130T\u0130\u015e RAPORU ---\n",
        "    print(f\"\\n{lang_code} D\u0130L\u0130 E\u011e\u0130T\u0130M\u0130 BA\u015eARIYLA TAMAMLANDI.\")\n",
        "    print(f\"Son Model: {model_path}\")\n",
        "\n",
        "\n",
        "# --- ANA E\u011e\u0130T\u0130M BORU HATTI ---\n",
        "\n",
        "def run_training_pipeline():\n",
        "    \"\"\"SADECE TR dili i\u00e7in e\u011fitim s\u00fcrecini y\u00f6netir.\"\"\"\n",
        "\n",
        "    global_start_time = time.time()\n",
        "    os.makedirs(DRIVE_MODEL_DIR, exist_ok=True)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Kullan\u0131lan Cihaz: {device}\")\n",
        "\n",
        "    all_language_stats = {}\n",
        "\n",
        "    # Sadece TR i\u00e7in e\u011fitim ba\u015flat\u0131l\u0131yor\n",
        "    for lang_code in ALL_LANG_CODES:\n",
        "        lang_start_time = time.time()\n",
        "\n",
        "        train_paths, test_paths, train_labels, test_labels, class_names = prepare_lang_data(ROOT_DIR, lang_code)\n",
        "\n",
        "        if train_paths is None:\n",
        "            continue\n",
        "\n",
        "        train_single_language_model(lang_code, train_paths, test_paths, train_labels, test_labels, class_names)\n",
        "\n",
        "        lang_duration = time.time() - lang_start_time\n",
        "        all_language_stats[lang_code] = format_duration(lang_duration)\n",
        "\n",
        "        print(f\"\\n--- {lang_code} \u0130\u015flemleri Tamamland\u0131. Toplam S\u00fcre: {all_language_stats[lang_code]} ---\\n\")\n",
        "\n",
        "\n",
        "    # --- GLOBAL B\u0130T\u0130\u015e RAPORU ---\n",
        "    global_duration = time.time() - global_start_time\n",
        "    formatted_global_duration = format_duration(global_duration)\n",
        "\n",
        "    print(\"\\n\\n\" + \"=\"*70)\n",
        "    print(\"T\u00dcRK\u00c7E D\u0130L MODEL\u0130N\u0130N E\u011e\u0130T\u0130M\u0130 BA\u015eARIYLA TAMAMLANDI.\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"T\u00fcm E\u011fitim S\u00fcresi: {formatted_global_duration}\")\n",
        "\n",
        "# E\u011fitimi ba\u015flat\n",
        "if __name__ == '__main__':\n",
        "    run_training_pipeline()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711,
          "referenced_widgets": [
            "0c74e7f2ba9345de8d2e4a235edeffbb",
            "bee8db59800840d3a82c94ea0f8f5ab9",
            "003690d14a944274a8f345f22a82a774",
            "e6dd645e4ee34f61b32c87c738bbd73e",
            "fbd1ed5fab344825862d66164d615104",
            "aec8464febb04c4f98540f3ef8f902a8",
            "1256e58c8cc74deebc86a47d91799680",
            "cafb0ac332714a52881c8bc2c71785fc",
            "e8a590c25df74af6b307f53a56b40da7",
            "f5b9820830f64da48c94ad6a4f74d510",
            "f6ac8f52bf4d46adb85a37e8ccc27277"
          ]
        },
        "id": "xYqgDZsehP9B",
        "outputId": "635fe4ce-9260-4fbe-b059-fb1169b5285d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kullan\u0131lan Cihaz: cuda\n",
            "\n",
            "Veri Kontrol\u00fc: 'TR' i\u00e7in Toplam 34500 resim bulundu.\n",
            "\n",
            "TR i\u00e7in 23 s\u0131n\u0131f haritas\u0131 '/content/drive/MyDrive/vmamba_models/class_map_TR.json' dosyas\u0131na kaydedildi.\n",
            "\n",
            "==================== TR D\u0130L\u0130 \u0130\u00c7\u0130N E\u011e\u0130T\u0130M BA\u015eLIYOR (23 S\u0131n\u0131f) ====================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/354M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model, '/content/drive/MyDrive/vmamba_models/vmamba_TR_epoch_1.pth' olarak kaydedildi.\n",
            "Epoch 1 tamamland\u0131. Do\u011fruluk: 0.9967. Detayl\u0131 rapor '/content/drive/MyDrive/vmamba_models/vmamba_TR_epoch_details.txt' dosyas\u0131na eklendi.\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model, '/content/drive/MyDrive/vmamba_models/vmamba_TR_epoch_2.pth' olarak kaydedildi.\n",
            "Epoch 2 tamamland\u0131. Do\u011fruluk: 0.9907. Detayl\u0131 rapor '/content/drive/MyDrive/vmamba_models/vmamba_TR_epoch_details.txt' dosyas\u0131na eklendi.\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model, '/content/drive/MyDrive/vmamba_models/vmamba_TR_epoch_3.pth' olarak kaydedildi.\n",
            "Epoch 3 tamamland\u0131. Do\u011fruluk: 0.9784. Detayl\u0131 rapor '/content/drive/MyDrive/vmamba_models/vmamba_TR_epoch_details.txt' dosyas\u0131na eklendi.\n",
            "--------------------------------------------------\n",
            "\n",
            "TR D\u0130L\u0130 E\u011e\u0130T\u0130M\u0130 BA\u015eARIYLA TAMAMLANDI.\n",
            "Son Model: /content/drive/MyDrive/vmamba_models/vmamba_TR_epoch_3.pth\n",
            "\n",
            "--- TR \u0130\u015flemleri Tamamland\u0131. Toplam S\u00fcre: 04 saat, 27 dakika, 03 saniye ---\n",
            "\n",
            "\n",
            "\n",
            "======================================================================\n",
            "T\u00dcRK\u00c7E D\u0130L MODEL\u0130N\u0130N E\u011e\u0130T\u0130M\u0130 BA\u015eARIYLA TAMAMLANDI.\n",
            "======================================================================\n",
            "T\u00fcm E\u011fitim S\u00fcresi: 04 saat, 27 dakika, 03 saniye\n"
          ]
        }
      ]
    }
  ]
}